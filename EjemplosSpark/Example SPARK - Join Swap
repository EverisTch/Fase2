You have given a files as below

EmployeeName.csv
E01,Lokesh
E02,Bhupesh
E03,Amit
E04,Ratan
E05,Dinesh
E06,Pavan
E07,Tejas
E08,Sheela
E09,Kumar
E10,Venkat

EmployeeSalary.csv
E01,50000
E02,50000
E03,45000
E04,45000
E05,50000
E06,45000
E07,50000
E08,10000
E09,10000
E10,10000

Now write a Spark code in scala which will load these two files from hdfs and join the same, and produce the (name, salary) values.
and save the data in multiple file group by salary (means each file will have name of employees with same salary). Make sure file name include salary as well.

val EmployeeName   = sc.textFile("/user/cloudera/spark5/EmployeeName.csv").map(x => (x.split(",")(0), x.split(",")(1)))
val EmployeeSalary = sc.textFile("/user/cloudera/spark5/EmployeeSalary.csv").map(x => (x.split(",")(0), x.split(",")(1)))

val joinedData     = EmployeeName.join(EmployeeSalary).values
val swapped        = joinedData.map(x => x.swap)

# group by keys
val grpbykeys 	   = swapped.groupByKey().collect()

# now create RDD for values collections
val rddByKey 	   = grpbykeys.map{ case (k,v) => k -> sc.makeRDD(v.toSeq)}

# save the output as a text file
rddByKey.foreach{ case (k, rdd) => rdd.saveAsTextFile("/user/cloudera/spark5/Employee"+k)}

